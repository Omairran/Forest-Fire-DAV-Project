# -*- coding: utf-8 -*-
"""DAV Project Phase 1-4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RLQ6CH7yxKpiazqNJz4DWV0LZTlXfjus

# Task
Visualizations: Create different types of plots (e.g., histograms, scatter plots, box plots,
etc.) to visualize your data
Handle Missing Values: Identify and address missing values.
• Remove Duplicates: Identify and remove any duplicate records.
• Outliers: Detect and handle outliers appropriately.
• Data Transformation: Apply necessary transformations (e.g., scaling, encoding
categorical variables)
Correlation Matrix: Calculate and visualize the correlation matrix.


apply all this on this data set and write code in this colab file

Here is all the data you need:
"forestfires.csv"

## Data loading

### Subtask:
Load the "forestfires.csv" dataset.

**Reasoning**:
Load the dataset and display the first 5 rows to verify.
"""

import pandas as pd

try:
    df = pd.read_csv('forestfires.csv')
    display(df.head())
except FileNotFoundError:
    print("Error: 'forestfires.csv' not found.")
    df = None

"""## Data exploration

### Subtask:
Explore the loaded dataset to understand its characteristics.

**Reasoning**:
I need to explore the dataset by examining its shape, data types, descriptive statistics, missing values, duplicates, and unique values in categorical columns.
"""

# Examine the shape of the DataFrame
print("Shape of the DataFrame:", df.shape)

# Display data types of each column
print("\nData Types:")
print(df.info())

# Generate descriptive statistics for numerical features
print("\nDescriptive Statistics:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Identify and count duplicate rows
print("\nDuplicate Rows:")
print(f"Number of duplicate rows: {df.duplicated().sum()}")

# Print unique values for categorical columns
print("\nUnique Values for Categorical Columns:")
print("Month:", df['month'].unique())
print("Day:", df['day'].unique())

"""## Data cleaning

### Subtask:
Clean the data by removing duplicate rows and handling outliers in the 'area' column.

**Reasoning**:
Remove duplicate rows and handle outliers in the 'area' column using IQR method.
"""

# Remove duplicate rows
df = df.drop_duplicates(keep='first')

# Handle outliers in the 'area' column using the IQR method
Q1 = df['area'].quantile(0.25)
Q3 = df['area'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out outliers
df = df[(df['area'] >= lower_bound) & (df['area'] <= upper_bound)]

"""## Data analysis

### Subtask:
Calculate the correlation matrix for the numerical features in the cleaned dataset and identify highly correlated features.

**Reasoning**:
Calculate the correlation matrix for the numerical features and identify highly correlated features.
"""

# Select numerical features, excluding 'X', 'Y', 'month', and 'day'
numerical_features = ['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area']
numerical_df = df[numerical_features]

# Calculate the correlation matrix
correlation_matrix = numerical_df.corr()

# Print the correlation matrix
display(correlation_matrix)

# Identify highly correlated features (example threshold: 0.7)
threshold = 0.7
highly_correlated_features = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            highly_correlated_features.append(
                (correlation_matrix.columns[i],
                 correlation_matrix.columns[j],
                 correlation_matrix.iloc[i, j])
            )
print("\nHighly Correlated Feature Pairs (threshold = {}):".format(threshold))
for feature1, feature2, correlation in highly_correlated_features:
    print(f"{feature1} - {feature2}: {correlation:.2f}")

"""## Data wrangling

### Subtask:
Prepare the data for visualization and modeling by scaling numerical features and encoding categorical features.

**Reasoning**:
Scale numerical features using StandardScaler and encode categorical features using one-hot encoding, then combine the results into a single DataFrame.
"""

from sklearn.preprocessing import StandardScaler
import pandas as pd

# Scale numerical features
numerical_features = ['FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area']
scaler = StandardScaler()
df_numerical_scaled = pd.DataFrame(scaler.fit_transform(df[numerical_features]), columns=numerical_features, index=df.index)

# Encode categorical features
df_categorical_encoded = pd.get_dummies(df[['month', 'day']], drop_first=True)

# Combine DataFrames
df_processed = pd.concat([df_numerical_scaled, df_categorical_encoded], axis=1)
display(df_processed.head())

"""## Data visualization

### Subtask:
Visualize the data using histograms, scatter plots, box plots, and a heatmap of the correlation matrix.

**Reasoning**:
Create visualizations (histograms, scatter plots, box plots, and a correlation matrix heatmap) using the processed dataframe `df_processed`.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Histograms
plt.figure(figsize=(15, 10))
for i, col in enumerate(df_processed.select_dtypes(include=['float64']).columns):
    plt.subplot(3, 3, i + 1)
    plt.hist(df_processed[col], bins=20, color='skyblue', edgecolor='black')
    plt.title(col)
plt.tight_layout()
plt.show()


# Scatter Plots (example: temp vs. area, colored by area)
plt.figure(figsize=(8, 6))
plt.scatter(df_processed['temp'], df_processed['area'], c=df_processed['area'], cmap='viridis')
plt.xlabel('Temperature')
plt.ylabel('Area')
plt.title('Temperature vs. Area')
plt.colorbar(label='Area')
plt.show()


# Box Plots
plt.figure(figsize=(15, 10))
for i, col in enumerate(df_processed.select_dtypes(include=['float64']).columns):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(y=df_processed[col], color='lightgreen')
    plt.title(col)
plt.tight_layout()
plt.show()


# Correlation Matrix Heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(df_processed.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

"""## Summary:

### 1. Q&A

* **Are there any missing values in the dataset?** No, the initial data exploration revealed no missing values.
* **Are there any duplicate rows in the dataset?** Yes, four duplicate rows were identified and removed during the data cleaning process.
* **Are there any highly correlated features in the dataset (correlation > 0.7)?** No, no highly correlated features were found using a correlation threshold of 0.7.

### 2. Data Analysis Key Findings

* **Outliers in 'area':** Outliers were detected in the 'area' column and handled using the IQR method.  The exact number of outliers removed is not specified in the provided information, but the process removed rows where the 'area' values were outside a calculated upper and lower bound.
* **Data Transformation:** Numerical features ('FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area') were scaled using `StandardScaler`. Categorical features ('month', 'day') were one-hot encoded using `pd.get_dummies` with `drop_first=True`.


### 3. Insights or Next Steps

* **Further Investigate Feature Relationships:** While no highly correlated features were found above a threshold of 0.7, a more in-depth exploration of the relationships between features might reveal subtle dependencies not captured by the simple correlation analysis. This could involve more advanced statistical tests or visualizations beyond simple scatter plots.
* **Model Training and Evaluation:**  The data has been cleaned and preprocessed; the next step is to train a machine learning model to predict forest fire area or other relevant variables.  Choose an appropriate model based on the problem and evaluate its performance using relevant metrics.

"""